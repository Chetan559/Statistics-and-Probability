{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e6e830",
   "metadata": {},
   "source": [
    "## Mathematical Transformation\n",
    "\n",
    "In the real world, we do not get any normally distributed data, so we have to think about how to convert data that looks normal in Normal Distribution.\n",
    "\n",
    "So we use different TRANSFORMATION for this.\n",
    "\n",
    "- Transformations are nothing but mathematical transforms where by applying some mathematical operation we can try to convert our distribution into a Normal Distribution.\n",
    "\n",
    "There are so many transformations, some of them are:\n",
    "- Log Transform  \n",
    "- Box-Cox Transform  \n",
    "- Reciprocal Transform  \n",
    "- Power Transform  \n",
    "- Yeo-Jonson Transform  \n",
    "\n",
    "### What happens when we apply this transformation? How does the performance of the Model improve?\n",
    "\n",
    "- The answer is our data distribution the PDF (Probability Density Function) converts into Normal Distribution.\n",
    "\n",
    "### Why do we want to transform into Normal distribution? What Is the reason or benefit behind doing this?\n",
    "\n",
    "- As we know statistics is the mother field of Machine Learning, and whenever any statistician works on any problem whatever complex problem upon which he is working anywhere he finds normal distribution he becomes happy, now can imagine how important is Normal Distribution.\n",
    "- By looking at the normally distributed data statisticians started feeling that they could solve the problem.\n",
    "- In terms of machine learning, we have some algorithms like ‚ÄúLinear Regression‚Äù, and ‚ÄúLogistic Regression‚Äù We work assuming that the data we are using is Normally distributed data.\n",
    "- If we do not have normal data we want to make them normal. Some other machine learning algorithms like ‚ÄúDecision Tree‚Äù, and ‚ÄúRandom Forest‚Äù give a dam, which means they don‚Äôt get bothered about the distribution type of data.\n",
    "\n",
    "In our machine learning library, we have inside sklearn three most used transformers given:\n",
    "\n",
    "1. Function Transformer  \n",
    "   (can do inside ‚Äî Log Transform, Square/Square root Transform, Reciprocal Transform, any custom function also)\n",
    "\n",
    "2. Power Transformer  \n",
    "   (can be done inside BOX-COX, YOE-JHONSON)\n",
    "\n",
    "3. Quantile Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ec793d",
   "metadata": {},
   "source": [
    "# Function Transformation\n",
    "\n",
    "## 1. Log Transformation\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*hd5oOa6YFMxkLvMNQlM-9Q.png\" width=\"300px\"><br>\n",
    "  <em>Right skewed to Normal distributed</em>\n",
    "</p>\n",
    "\n",
    "If we want to apply LOG TRANSFORM we apply log.\n",
    "\n",
    "- Ex: Suppose we have an age column from the Titanic dataset, and we want to apply a logarithmic transformation. Then, we take the logarithm for all values inside the age column. We can choose to use it either log base 2 or log base 10, depending on our preference. By taking the logarithm, the data will be converted into a normal distribution, although not completely, it will be improved from the current situation.\n",
    "\n",
    "#### When to use Log Transform:-\n",
    "\n",
    "- We can not apply log transform upon Negative values as we can not take the Log of negative values.\n",
    "- If we have RIGHT SKEWED data by applying log transform it shifts this right skewed into centre.\n",
    "- By applying Log we convert a very big range into an equivalent scale, that‚Äôs why we get normal values and everything looks linear, so linear models like linear regression or logistic regression perform better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897fe319",
   "metadata": {},
   "source": [
    "## 2. Reciprocal (1/x) Transformation\n",
    "\n",
    "In 1/x all big values will convert into small values and small values convert into big values.  \n",
    "This is a very different transform, sometimes we use this.\n",
    "\n",
    "## 3. Square (x¬≤) Transformation\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:480/format:webp/1*CbcBp8OrbZiVc38sp-2uNw.png\" width=\"300px\"><br>\n",
    "  <em>Square</em>\n",
    "</p>\n",
    "\n",
    "- (x) √ó (x) (APPLY WHEN WE HAVE LEFT SKEWED DATA)\n",
    "- This is especially used for ‚ÄúLEFT SKEWED DATA‚Äù\n",
    "\n",
    "## 4. Sqrt Transformation\n",
    "\n",
    "To be honest, it is not very useful, but we can try this.\n",
    "\n",
    "**Note:**  \n",
    "Transformation can not apply if we have missing value in data so before applying transformation we are supposed to deal with missing value.\n",
    "\n",
    "### Example\n",
    "\n",
    "**Titanic dataset:**  \n",
    "Columns taken ‚Äúage‚Äù, ‚Äúfare‚Äù, and ‚Äúsurvived‚Äù will check the before-transformation and after-transformation differences upon the Titanic dataset.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# importing all necessary libraries\n",
    "```\n",
    "```python\n",
    "# Titanic dataset with 3 columns uploaded \n",
    "df = pd.read_csv('train.csv',usecols=['Age','Fare','Survived'])\n",
    "\n",
    "# checking Null Values\n",
    "df.isnull().sum()\n",
    "\n",
    "Output:\n",
    "Survived      0\n",
    "Age         177\n",
    "Fare          0\n",
    "dtype: int64\n",
    "```\n",
    "```python\n",
    "# Replacing Nullvalues with Mean\n",
    "df['Age'].fillna(df['Age'].mean(),inplace=True)\n",
    "\n",
    "# splitting the data into Input and target variables\n",
    "X = df.iloc[:,1:3]\n",
    "y = df.iloc[:,0]\n",
    "\n",
    "# splitting the dataste into training and testset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "```\n",
    "```python\n",
    "# applyied PDF & QQ Plot Before Transformation to check Normality\n",
    "\n",
    "# Age Column PDF & QQ Plot\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(121)\n",
    "sns.distplot(X_train['Age'])\n",
    "plt.title('Age PDF')\n",
    "\n",
    "plt.subplot(122)\n",
    "stats.probplot(X_train['Age'], dist=\"norm\", plot=plt)\n",
    "plt.title('Age QQ Plot')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*zvNHsUkW_CM1q0YK_DVwZA.png\" width=\"300px\"><br>\n",
    "  <em>PDF & QQ Plot before Transformation ‚Äî Titanic Age Column</em>\n",
    "</p>\n",
    "\n",
    "```python\n",
    "# PDF & QQ Plot upon Fare Column Before Transformation\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(121)\n",
    "sns.distplot(X_train['Fare'])\n",
    "plt.title('Fare PDF')\n",
    "\n",
    "plt.subplot(122)\n",
    "stats.probplot(X_train['Fare'], dist=\"norm\", plot=plt)\n",
    "plt.title('Fare QQ Plot')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*bVEqbN_LMtdf-yeiUXR01g.png\" width=\"300px\"><br>\n",
    "  <em>PDF & QQ lot Before Transformation Upon Titanic Fare Column</em>\n",
    "</p>\n",
    "\n",
    "```python\n",
    "# applying linear and tree based algorithm\n",
    "clf = LogisticRegression()\n",
    "clf2 = DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "clf2.fit(X_train,y_train)\n",
    "    \n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred1 = clf2.predict(X_test)\n",
    "    \n",
    "print(\"Accuracy LR\",accuracy_score(y_test,y_pred))\n",
    "print(\"Accuracy DT\",accuracy_score(y_test,y_pred1))\n",
    "\n",
    "output:-\n",
    "Accuracy LR 0.6480446927374302\n",
    "Accuracy DT 0.6480446927374302\n",
    "```\n",
    "```python\n",
    "# Now applying transformation \"log transformation\" technique upon column 'age' and 'fare' \n",
    "#to see it will tend towards normal distribution or not\n",
    "trf = FunctionTransformer(func=np.log1p)\n",
    "\n",
    "X_train_transformed = trf.fit_transform(X_train)\n",
    "X_test_transformed = trf.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf2 = DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(X_train_transformed,y_train)\n",
    "clf2.fit(X_train_transformed,y_train)\n",
    "    \n",
    "y_pred = clf.predict(X_test_transformed)\n",
    "y_pred1 = clf2.predict(X_test_transformed)\n",
    "    \n",
    "print(\"Accuracy LR\",accuracy_score(y_test,y_pred))\n",
    "print(\"Accuracy DT\",accuracy_score(y_test,y_pred1))\n",
    "\n",
    "Output:-\n",
    "LR 0.678027465667915\n",
    "DT 0.6610736579275905\n",
    "\n",
    "\n",
    "# for assurance of result doing Cross Validation 10 times\n",
    "\n",
    "X_transformed = trf.fit_transform(X)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf2 = DecisionTreeClassifier()\n",
    "\n",
    "print(\"LR\",np.mean(cross_val_score(clf,X_transformed,y,scoring='accuracy',cv=10)))\n",
    "print(\"DT\",np.mean(cross_val_score(clf2,X_transformed,y,scoring='accuracy',cv=10)))\n",
    "\n",
    "output:- \n",
    "LR 0.678027465667915\n",
    "DT 0.6565917602996254\n",
    "```\n",
    "```python\n",
    "plt.figure(figsize=(14,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "stats.probplot(X_train['Fare'], dist=\"norm\", plot=plt)\n",
    "plt.title('Fare Before Log')\n",
    "\n",
    "plt.subplot(122)\n",
    "stats.probplot(X_train_transformed['Fare'], dist=\"norm\", plot=plt)\n",
    "plt.title('Fare After Log')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*7I_ozvl0vOYeV9LFtCGjsw.png\" width=\"300px\"><br>\n",
    "  <em>Fare data point distribution Before and after transformation</em>\n",
    "</p>\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(14,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "stats.probplot(X_train['Age'], dist=\"norm\", plot=plt)\n",
    "plt.title('Age Before Log')\n",
    "\n",
    "plt.subplot(122)\n",
    "stats.probplot(X_train_transformed['Age'], dist=\"norm\", plot=plt)\n",
    "plt.title('Age After Log')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Cr7gBM5HjCWmASK_gShX3g.png\" width=\"300px\"><br>\n",
    "  <em>Age Data Point before and after Transformation/em>\n",
    "</p>\n",
    "\n",
    "\n",
    "### Observation\n",
    "\n",
    "- The distribution of the age column before transformation also appears to be normally distributed. In the QQ Plot, most of the data points are located above the line, with only a few deviations from the line.\n",
    "- However, the fare column distribution clearly exhibits a right-skewed pattern, indicating the need for a log transformation. Before the transformation, we applied two different algorithms: a linear-based algorithm (Logistic Regression) and a tree-based algorithm (Decision Tree), to assess accuracy. The Logistic Regression model achieved 64% accuracy, while the Decision Tree model achieved 68% accuracy.\n",
    "- Next, we applied a log transformation to both columns using `np.log1p` instead of `np.log` to avoid potential issues with zero values in the dataset. Interestingly, the transformation had a more significant impact on the linear model compared to the tree-based model, as decision trees inherently segment the data.\n",
    "- To validate the transformation‚Äôs effectiveness, we conducted a cross-validation with 10 iterations, resulting in a consistent 67% accuracy for the logistic regression model. This outcome confirms that the transformation successfully improved the data distribution.\n",
    "- Upon visualization, we observed that the fare column, which was previously far from a normal distribution, moved closer to normality after transformation. Conversely, the age column, which initially exhibited a satisfactory distribution, performed poorly after transformation. This suggests that transformation may not be necessary when the data distribution is already close to normal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3edc35",
   "metadata": {},
   "source": [
    "# Power Transformation\n",
    "\n",
    "## 1. Box-Cox Trnsformation\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*_aCKBEOaJ54i-L6-bG36vA.png\" width=\"300px\"><br>\n",
    "  <em>PDF & QQ Plot before Transformation ‚Äî Titanic Age Column</em>\n",
    "</p>\n",
    "\n",
    "- Based on two computer scientists Box and Cox this transform name.\n",
    "- This is ‚ÄúGeneral Transform‚Äù which can apply upon any dataset.\n",
    "- By using BOX_COX TRANSFORM we can convert any type of distribution into ‚ÄúNORMAL DISTRIBUTION‚Äù\n",
    "- This is general transformation and its special case is ‚ÄúLog transform‚Äù and ‚ÄúSquare Root Transform‚Äù\n",
    "\n",
    "$$\n",
    "y(\\lambda) = \n",
    "\\begin{cases} \n",
    "\\frac{y^\\lambda - 1}{\\lambda}, & \\text{if } \\lambda \\neq 0 \\\\\n",
    "\\ln(y), & \\text{if } \\lambda = 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Parameters and Constraints\n",
    "\n",
    "* **$y$**: The original data value.  \n",
    "  **Note:** The Box-Cox transformation requires all $y$ values to be strictly positive ($y > 0$).\n",
    "* **$\\lambda$**: The transformation parameter. Common values include:\n",
    "  * **$\\lambda = 1$**: No transformation (identity).\n",
    "  * **$\\lambda = 0$**: Logarithmic transformation ($\\ln(y)$).\n",
    "  * **$\\lambda = 0.5$**: Square root transformation ($\\sqrt{y}$).\n",
    "  * **$\\lambda = -1$**: Reciprocal transformation ($1/y$).\n",
    "* **$\\ln$**: The natural logarithm.\n",
    "\n",
    "Here, X represents the input variable on which we apply the transformation (X^Œª ‚àí 1 / Œª), depending on the value of lambda.\n",
    "\n",
    "Lambda, denoted by Œª, essentially signifies the power to which X is raised. We aim to determine the optimal power for X, which could be any value such as X squared, X cubed, or even X raised to the power of 1.5.\n",
    "\n",
    "The exponent here is a variable called lambda that varies over the range of -5 to 5 and in the process of searching,  \n",
    "We examine all values of lambda. So basically we try randomly all values between -5 to 5 to check which one is the best.\n",
    "\n",
    "Finally, we choose the optimal value (resulting in the best approximation to a normal distribution) for your variable. It means the value gives the best normal distribution upon data that the lambda value we choose for transformation.  \n",
    "The lambda value which gives the best normal distribution will take those lambda values.\n",
    "\n",
    "There are two techniques to find this:-\n",
    "\n",
    "A- MAXIMUM LIKELIHOOD (used in Logistic Regression)\n",
    "\n",
    "B- BAYESIAN STATISTICS (it comes under ‚ÄòInferential statistics‚Äô)\n",
    "\n",
    "Boxcox transform is strictly applicable to only NUMBERS that are GREATER THAN ZERO.\n",
    "\n",
    "N > 0 is only applicable for box-cox, Zero, and negative won't work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732ac569",
   "metadata": {},
   "source": [
    "## 2. Yeo-Johnson Transformation\n",
    "\n",
    "$$\n",
    "y(\\lambda) = \n",
    "\\begin{cases} \n",
    "\\frac{(y + 1)^\\lambda - 1}{\\lambda}, & \\text{if } \\lambda \\neq 0, y \\ge 0 \\\\\n",
    "\\ln(y + 1), & \\text{if } \\lambda = 0, y \\ge 0 \\\\\n",
    "\\frac{-[(-y + 1)^{2-\\lambda} - 1]}{2 - \\lambda}, & \\text{if } \\lambda \\neq 2, y < 0 \\\\\n",
    "-\\ln(-y + 1), & \\text{if } \\lambda = 2, y < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- It is used to solve the restrictions of Box-Cox Transform. As we know Box-Cox can not apply to Zero and Negative Numbers.\n",
    "- This is also found by 2 computer scientists named Yeo, Johnson.\n",
    "- This is a kind of VARIATION of Box-Cox Transform.\n",
    "- All variations come from that it can work upon ‚ÄúNegative‚Äù and ‚ÄúZero‚Äù values.\n",
    "- This transformation is somewhat of an adjustment to the Box-Cox transformation, by which we can apply it to negative numbers.\n",
    "\n",
    "**Note:**  \n",
    "We have to use the ‚ÄúPOWER TRANSFORM‚Äù class of scikit-learn and we get both implementations inside that, and where we feel like ‚Äúdistribution is not normal‚Äù, we are using such algorithm which works well upon normally distributed data like Linear Regression, Logistic Regression, KNN, then we need to apply both transformations. In the end, we need to do only parameter training and we get to know whether we should apply Box-Cox or we should go with Yeo-Johnson.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfecd44b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Credits\n",
    "\n",
    "**Prepared by:**  \n",
    "**Chetan Sharma**  \n",
    "AIML / Data Science Notes  \n",
    "\n",
    "üîó **GitHub:** [github.com/Chetan559](https://github.com/Chetan559)  \n",
    "üåê **Portfolio:** [chetan559.github.io](https://chetan559.github.io)  \n",
    "üíº **LinkedIn:** [linkedin.com/in/sharma-chetan-k](https://www.linkedin.com/in/sharma-chetan-k/)  \n",
    "\n",
    "These notes were compiled for learning, revision, and academic understanding. \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
