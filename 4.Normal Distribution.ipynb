{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90fb2951",
   "metadata": {},
   "source": [
    "## How to use PDF in Data Science?\n",
    "\n",
    "definition: A Probability Density Function (PDF) describes the probability distribution of a continuous random variable. The area under the curve between two points gives the probability for that interval.\n",
    "\n",
    "key concept: PDF height shows relative likelihood. Higher PDF = more likely values in that region. Total area under curve = 1.\n",
    "\n",
    "\n",
    "### Main Use Cases\n",
    "\n",
    "#### 1. **Understanding Data Distribution**\n",
    "- Visualize data shape using KDE (Kernel Density Estimation)\n",
    "- Identify if normal, skewed, bimodal, etc.\n",
    "- Example: `sns.kdeplot(data['salary'])` shows salary distribution\n",
    "\n",
    "#### 2. **Detecting Outliers**\n",
    "- Low PDF values indicate rare/unusual data points\n",
    "- Useful for fraud detection, anomaly detection\n",
    "\n",
    "#### 3. **Feature Engineering**\n",
    "- Check if features are normally distributed\n",
    "- Apply transformations (log, sqrt) if skewed\n",
    "- Re-check PDF after transformation\n",
    "\n",
    "#### 4. **Model Validation**\n",
    "- Verify model assumptions (e.g., normal residuals in regression)\n",
    "- Compare empirical PDF with theoretical PDF\n",
    "\n",
    "#### 5. **Probability Calculations**\n",
    "- Find P(a < X < b) = area under curve from a to b\n",
    "- Calculate percentiles and quantiles\n",
    "- Example: Probability customer spends $50-$100\n",
    "\n",
    "#### 6. **Risk Assessment**\n",
    "- Quantify uncertainty in predictions\n",
    "- Calculate Value at Risk (VaR) in finance\n",
    "- Provide probabilistic forecasts instead of point estimates\n",
    "\n",
    "\n",
    "### Common PDFs in Data Science\n",
    "\n",
    "- **Normal**: Bell-shaped, symmetric (heights, test scores)\n",
    "- **Log-Normal**: Right-skewed (income, stock prices)\n",
    "- **Exponential**: Time between events (customer arrivals)\n",
    "- **Uniform**: All values equally likely (random generation)\n",
    "\n",
    "\n",
    "### Quick Workflow\n",
    "```python\n",
    "# 1. Visualize PDF\n",
    "sns.kdeplot(data=df['variable'])\n",
    "\n",
    "# 2. Identify distribution type\n",
    "# 3. Apply appropriate techniques based on shape\n",
    "# 4. Make data-driven decisions\n",
    "```\n",
    "\n",
    "note: PDF helps you understand data distribution, validate assumptions, detect outliers, and make probabilistic predictions. Always combine PDF visualization with other statistics for complete insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aaf115",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2D Density Plots\n",
    "\n",
    "definition: A visualization that shows the probability density of two continuous variables simultaneously. It displays where data points are concentrated in 2D space using colors or contours.\n",
    "\n",
    "also known as: Bivariate density plot, 2D KDE plot, density heatmap\n",
    "\n",
    "\n",
    "### How It Works\n",
    "\n",
    "- Extension of 1D PDF to two dimensions\n",
    "- Color intensity or contour lines represent density\n",
    "- Darker/brighter colors = higher concentration of data points\n",
    "- Lighter colors = fewer data points\n",
    "- Smooth representation of data distribution\n",
    "\n",
    "\n",
    "### Types of 2D Density Plots\n",
    "\n",
    "#### 1. **Contour Plot**\n",
    "- Uses lines (contours) to show regions of equal density\n",
    "- Like elevation lines on a topographic map\n",
    "- Each line represents same density level\n",
    "- Closer lines = steeper density change\n",
    "\n",
    "#### 2. **Filled Contour Plot**\n",
    "- Contour plot with colored regions between lines\n",
    "- Easier to see density gradients\n",
    "- Color scale shows density intensity\n",
    "\n",
    "#### 3. **Heatmap Density Plot**\n",
    "- Smooth color gradient showing density\n",
    "- No distinct contour lines\n",
    "- Most intuitive for quick interpretation\n",
    "\n",
    "\n",
    "### When to Use\n",
    "\n",
    "**Use 2D density plots when:**\n",
    "- You have many overlapping points in scatterplot\n",
    "- Want to see concentration patterns in 2D space\n",
    "- Need to visualize distribution of two continuous variables\n",
    "- Scatterplot is too cluttered (thousands of points)\n",
    "- Identifying clusters or modes in bivariate data\n",
    "\n",
    "**Don't use when:**\n",
    "- You have few data points (scatterplot is better)\n",
    "- Variables are categorical\n",
    "- You need to see individual data points\n",
    "\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Handles overplotting (overlapping points) well\n",
    "- Shows density patterns clearly with many data points\n",
    "- Reveals multimodal distributions (multiple peaks)\n",
    "- Smooth, professional appearance\n",
    "- Reduces visual clutter\n",
    "\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- Loses information about individual points\n",
    "- Requires sufficient data for accurate estimation\n",
    "- Bandwidth selection affects appearance\n",
    "- Can hide outliers\n",
    "- Harder to see exact values\n",
    "\n",
    "\n",
    "### Python Implementation\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Method 1: Seaborn KDE plot (smooth density)\n",
    "sns.kdeplot(data=df, x='variable1', y='variable2', fill=True, cmap='Blues')\n",
    "\n",
    "# Method 2: Hexbin plot (alternative)\n",
    "plt.hexbin(df['variable1'], df['variable2'], gridsize=30, cmap='YlOrRd')\n",
    "\n",
    "# Method 3: 2D histogram (discrete bins)\n",
    "plt.hist2d(df['variable1'], df['variable2'], bins=50, cmap='viridis')\n",
    "```\n",
    "\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **Single peak**: Unimodal distribution, one main cluster\n",
    "- **Multiple peaks**: Multiple clusters or subgroups\n",
    "- **Elongated shape**: Strong correlation between variables\n",
    "- **Circular shape**: Little to no correlation\n",
    "- **Diagonal pattern**: Positive or negative correlation\n",
    "\n",
    "\n",
    "### Combining with Scatterplot\n",
    "\n",
    "Best practice: Overlay density with scatterplot\n",
    "```python\n",
    "sns.kdeplot(data=df, x='x', y='y', fill=True, alpha=0.5)\n",
    "sns.scatterplot(data=df, x='x', y='y', alpha=0.3)\n",
    "```\n",
    "Shows both overall density and individual points\n",
    "\n",
    "\n",
    "### Common Applications\n",
    "\n",
    "- Customer segmentation (age vs income)\n",
    "- Visualizing correlation with many data points\n",
    "- Geographic data (latitude vs longitude density)\n",
    "- Sensor data analysis (temperature vs pressure)\n",
    "- Bioinformatics (gene expression levels)\n",
    "\n",
    "note: 2D density plots are the 2D equivalent of histograms/KDE plots for 1D data. They're essential when scatterplots become too crowded with data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76eed34",
   "metadata": {},
   "source": [
    "## Normal Distribution\n",
    "\n",
    "It is also known as **â€˜Gaussian Distributionâ€™** or **â€˜Bell-Curvedâ€™**.\n",
    "\n",
    "- Normal Distribution is also part of **Parametric Density Estimation**.\n",
    "- It is a **Continuous Probability Distribution**, which means it is a **Probability Density Function (PDF)** that is **symmetrical around the mean**, and looks like a **bell-shaped curve**.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*G5J3SMzt2cmYDnBL6qu9MQ.jpeg\" width=\"400\">\n",
    "  <br>\n",
    "  <em>Normal Distribution</em>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "### In this image\n",
    "- At the center **â€˜0â€™** represents the **mean value**.\n",
    "- On the **Y-axis**, it represents **Probability Density**, while on the **X-axis** we have normal $x$ values.\n",
    "- The curve inclines towards the ends, called **â€˜Tailsâ€™** on both sides. The tail never touches the x-axis. It is **Asymptotic** in nature, which means it touches only when it reaches **Infinity**.\n",
    "- A basic summary of this graph is that in the bell-shaped curve, many points are scattered near the center (Mean), and some points are scattered near tails on both sides.\n",
    "- A curve that is high at points means there is a high density of data, and where the curve is low, there is a low density of data.\n",
    "- A normal distribution is characterized by two parameters: **â€˜$\\mu$â€™ (Mean)** and **â€˜$\\sigma$â€™ (Standard Deviation)**.\n",
    "- If we have the mean and standard deviation of any data and it follows normal distribution, then we can easily create a graph.\n",
    "- Mean explains the **Centre** of data and the Standard Deviation explains the **Spread** of the data.\n",
    "- A normal distribution is decided based on these two parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## Importance of Normal Distribution\n",
    "\n",
    "- Because it is widespread, many natural phenomena follow the pattern of normal distribution such as heights, weights of people, IQ score of the population, salary distribution, and more.\n",
    "- In statistics, over many years, researchers across different domains have collected and analyzed data. When they created PDFs for selected data, they consistently observed a bell-shaped curve resembling the normal distribution. Hence, this graph was considered important because it exists so frequently in nature, which is why it started being called the **Normal Distribution**.\n",
    "- Since then, many studies have been done on the normal distribution, and various aspects of this distribution are known.\n",
    "- Therefore, if our data follows a normal distribution, it becomes very easy to analyze due to the well-understood characteristics of the distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## PDF Equation of Normal Distribution\n",
    "\n",
    "$$f(x \\mid \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\, e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}$$\n",
    "\n",
    "**Where:**\n",
    "- $x$: The value of the random variable  \n",
    "- $\\mu$: The population mean (location of the peak)  \n",
    "- $\\sigma$: The population standard deviation (spread of the distribution)  \n",
    "- $\\sigma^2$: The variance  \n",
    "- $\\pi$: Approximately 3.14159  \n",
    "- $e$: The base of the natural logarithm, approximately 2.71828  \n",
    "\n",
    "---\n",
    "\n",
    "## Parameters of Normal Distribution\n",
    "\n",
    "If we change the values of $\\mu$ and $\\sigma$ in the equation of the normal distribution, it will affect the shape and position of the graph.\n",
    "\n",
    "### 1. Effect of changing â€˜$\\mu$â€™ (mean) â€” impact on bell curve position\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*f1P_NPS2Gr4Bh9GmHsq8kw.png\" width=\"400\">\n",
    "  <br>\n",
    "  <em>Change in position of normal distribution on x-axis with change in mean â€˜$\\mu$â€™</em>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "- If $\\mu$ moves from 0 towards the positive direction, the graph on the x-axis will start shifting towards the positive direction.\n",
    "- Conversely, if mean ($\\mu$) moves from 0 towards the negative direction, the graph on the x-axis will start shifting towards the negative direction.\n",
    "- Essentially, changing the mean $\\mu$ causes a **horizontal shift** of the graph along the x-axis.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Effect of changing â€˜$\\sigma$â€™ (standard deviation) â€” impact on bell curve shape\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*zlBeMCIRo8iiEq8dP4JZ_A.jpeg\" width=\"400\">\n",
    "  <br>\n",
    "  <em>Change in â€˜$\\sigma$â€™ impact on normal distribution</em>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "- The standard deviation $\\sigma$ influences the spread or dispersion of the data.\n",
    "- If we increase $\\sigma$, the height of the curve decreases slightly, resulting in a fatter shape. This means an increase in spread causes the curve to become broader on the x-axis, with data points more scattered and less steep at its peak.\n",
    "- Conversely, if we decrease $\\sigma$, the curve becomes narrower, indicating a smaller spread of data points around the mean. The height of the curve increases, giving it a more peaked and sharper shape.\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition\n",
    "\n",
    "This is the intuition of Normal Distribution based on changes in its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6cb156",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Standard Normal Variate\n",
    "\n",
    "- This is special case of normal distribution.  \n",
    "- It is also known as **â€˜Zâ€™**  \n",
    "- If **â€˜muâ€™** value is **â€˜Zeroâ€™** and standard deviation is **â€˜1â€™** which means normal distribution is **â€˜centerâ€™** then it is called **â€˜Standard Normal Distributionâ€™** and denoted by **â€˜Zâ€™**.  \n",
    "- Normal distribution: **X ~ N(Î¼ , Ïƒ)**  \n",
    "- Standard Normal Distribution: **Z ~ N(0, 1)**  \n",
    "\n",
    "---\n",
    "\n",
    "### Importance of Standard Normal Distribution\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.mathsisfun.com/data/images/normal-distrubution-large.svg\" width=\"300\">\n",
    "  <br>\n",
    "  <em>Standard Normal Distribution</em>\n",
    "</p>\n",
    "\n",
    "This is **â€˜Standard Normal Variateâ€™** graph as **Mean = 0** and **Standard Deviation = 1**\n",
    "\n",
    "- It allow us to compare different distribution with each other by converting them into standard normal distribution.  \n",
    "- Using standardized normal distribution we can calculate **â€˜probabilityâ€™** using **â€˜Standard tableâ€™**.  \n",
    "- We can calculate any probabilities because for standard normal variate we have all probabilities value available.  \n",
    "- In normal distribution equation, when we replace mean **â€˜muâ€™** with **0** and standard deviation **â€˜sigmaâ€™** with **1** we get standard normal distribution formula.  \n",
    "\n",
    "Press enter or click to view image in full size  \n",
    "\n",
    "---\n",
    "\n",
    "### Z-table\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*e_d_6cUOL8vr2YAaZ_GaoQ.png\" width=\"300\">\n",
    "  <br>\n",
    "  <em>Z-Score Standardization</em>\n",
    "</p>\n",
    "\n",
    "#### Importance of Z-scores\n",
    "\n",
    "1. Can be used to determine whether to accept or reject the Null Hypothesis.  \n",
    "2. Enables us to compare two scores that are from different samples having different mean and standard deviations.  \n",
    "3. To identify the Outliers  \n",
    "4. Calculate probabilities and percentiles using the standard normal distribution.  \n",
    "\n",
    "---\n",
    "\n",
    "### Z-score Formula\n",
    "\n",
    "The formula for calculating a z-score is  \n",
    "\n",
    "**Z = (x âˆ’ Î¼) / Ïƒ**  \n",
    "\n",
    "**Z = (datapoint â€” Mean) / Standard Deviation**\n",
    "\n",
    "Where:  \n",
    "- **x** is datapoint of interest  \n",
    "- **Î¼** is the population mean  \n",
    "- **Ïƒ** is the population standard deviation  \n",
    "\n",
    "**Note:** Alternatively if Population Mean and standard deviation is not present, we can use the sample mean and standard deviation.  \n",
    "\n",
    "For more on z-score refer:  \n",
    "https://medium.com/analytics-vidhya/z-score-in-detail-9dd0f0afa142  \n",
    "\n",
    "---\n",
    "\n",
    "### Empirical Rule\n",
    "\n",
    "This is the most important properties of normal distribution\n",
    "\n",
    "- Approximately **68%** of data falls within **1 standard deviation** from the mean.  \n",
    "- Around **95%** of data falls within **2 standard deviations**.  \n",
    "- Nearly **99.7%** of data falls within **3 standard deviations**.  \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*BvSWE3LGXBSn7z-IVElI-Q.png\" width=\"300\">\n",
    "  <br>\n",
    "  <em>Empirical Rule</em>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187cfee3",
   "metadata": {},
   "source": [
    "## Properties of Normal Distribution\n",
    "\n",
    "### 1. Symmetricity\n",
    "\n",
    "It means normal distribution is **â€˜Symmetric around the meanâ€™**, its like mirror image.  \n",
    "If we know one side probability distribution we can easily calculate other side probability distribution too.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Measures of Central Tendency are Equal\n",
    "\n",
    "Mean, Median, Mode all equal for proper Normal Distribution.  \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:566/format:webp/1*RA7XLNHdsOWDx5eGKyKOEA.png\" width=\"300\">\n",
    "  <br>\n",
    "  <em>Mean = Median = Mode</em>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "### 3. The Area under the Curve\n",
    "\n",
    "Area under the curve is **â€˜1â€™** also this is True for any PDF (Probability Density Function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d1a26d",
   "metadata": {},
   "source": [
    "## Skewness\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Z0wk2ut5quB4bArWWFJ9Sw.png\" width=\"400\">\n",
    "  <br>\n",
    "  <em>Positive â€” Zero â€” Negative Skewness</em>\n",
    "</p>\n",
    "\n",
    "- Normal Distribution is **â€˜Symmetricâ€™** and **â€˜Skewnessâ€™** tells normal distribution is not symmetric.\n",
    "- It means data is leaning more towards one side if there is skewness.\n",
    "- How much data deviates from normal distribution can be found through skewness.\n",
    "- As we know in a **â€˜Symmetrical Distributionâ€™**, Mean, Median, and Mode are all **â€˜EQUALâ€™**.\n",
    "- But in the case of skewness, Mean, Median, and Mode are not equal, and one tail is longer than the other.\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Skewness\n",
    "\n",
    "There can be two types of skewness:\n",
    "\n",
    "#### Positive Skewness\n",
    "When the tail of the distribution is longer on the **â€˜Right sideâ€™**, it is called **â€˜Positive Skewedâ€™** or **â€˜Right Skewedâ€™**.  \n",
    "In this case, the Mean is greater than the Median and Mode:\n",
    "\n",
    "**Mean > Median > Mode**\n",
    "\n",
    "---\n",
    "\n",
    "#### Negative Skewness\n",
    "When the tail of the distribution is longer on the **â€˜Left sideâ€™**, it is called **â€˜Negative Skewedâ€™** or **â€˜Left Skewedâ€™**.  \n",
    "In this case, the Mean is less than the Median and Mode:\n",
    "\n",
    "**Mean < Median < Mode**\n",
    "\n",
    "> The greater the skew, the greater the distance between mean, median, and mode.\n",
    "\n",
    "---\n",
    "\n",
    "### How Skewness is Calculated?\n",
    "\n",
    "In statistics, there are four **â€˜Momentsâ€™**:\n",
    "1. First moment: **Mean**\n",
    "2. Second moment: **Variance**\n",
    "3. Third moment: **Skewness**\n",
    "4. Fourth moment: **Kurtosis**\n",
    "\n",
    "---\n",
    "\n",
    "### Skewness Formula\n",
    "\n",
    "The most common measure of skewness is the **Pearsonâ€™s Moment Coefficient of Skewness**.\n",
    "\n",
    "#### For Population Data ($\\gamma_1$):\n",
    "\n",
    "$$\n",
    "\\gamma_1 = E\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^3\\right] = \\frac{\\sum_{i=1}^{N} (x_i - \\mu)^3}{N\\sigma^3}\n",
    "$$\n",
    "\n",
    "#### For Sample Data ($g_1$):\n",
    "\n",
    "When working with a sample, the **Adjusted Fisher-Pearson Standardized Moment Coefficient** is typically used:\n",
    "\n",
    "$$\n",
    "g_1 = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^{n} \\left(\\frac{x_i - \\bar{x}}{s}\\right)^3\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation of Skewness\n",
    "\n",
    "- **Skewness = 0**: The distribution is perfectly symmetrical (like a Normal Distribution).\n",
    "- **Skewness > 0 (Positive Skew)**: The \"tail\" on the right side is longer or fatter. Most of the data is concentrated on the left.\n",
    "- **Skewness < 0 (Negative Skew)**: The \"tail\" on the left side is longer or fatter. Most of the data is concentrated on the right.\n",
    "\n",
    "---\n",
    "\n",
    "![Skewness Diagram](Assets/Img/skewness.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Note\n",
    "\n",
    "- The possible value of skewness range is from **$-3$ to $3$**.\n",
    "- Skewness values beyond the range of **$-2$ to $2$** are less frequently observed, indicating extreme departures from symmetry.\n",
    "- A value of **$0$** indicates a perfect symmetrical distribution.\n",
    "- A value between **$-0.5$ & $0$** or between **$0$ & $0.5$** indicates an **â€˜Approximately Symmetric Distributionâ€™**.\n",
    "- A value between **$-1$ & $-0.5$** or between **$0.5$ & $1$** indicates a **â€˜Moderately Skewed Distributionâ€™**.\n",
    "- A value between **$-1.5$ & $-1$** or between **$1$ & $1.5$** indicates a **â€˜Highly Skewed Distributionâ€™**.\n",
    "- A value less than **$-1.5$** or greater than **$1.5$** indicates an **â€˜Extremely Skewed Distributionâ€™**.\n",
    "- In real data, getting an exact **$0$** is very difficult; however, results around **$0.5$** or **$-0.5$** are often treated as a **Normal Distribution**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd0954f",
   "metadata": {},
   "source": [
    "## CDF of Normal Distribution\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*qA3ZWwljjwf0U-cQnT2-jw.png\" width=\"300\">\n",
    "  <br>\n",
    "  <em>CDF of Normal Distribution</em>\n",
    "</p>\n",
    "\n",
    "### In graph:-\n",
    "\n",
    "- The blue, red, and yellow lines all have a mean of **0** but differ in standard deviation.  \n",
    "- As the standard deviation increases, the curve shifts away from the mean.  \n",
    "- When the standard deviation is closer to the mean, the curve stays closer to the center.  \n",
    "- The center is where the mean equals **0**.  \n",
    "- The **Cumulative Distribution Function (CDF)** always explains the probability of being up to a certain point, denoted as **P(x < x)**.  \n",
    "- For a symmetric distribution, such as the normal distribution, this probability is **50%**.  \n",
    "- To find the CDF up to a particular point **â€˜xâ€™**, we integrate from **negative infinity** to that point.  \n",
    "- If we have the PDF equation and want to find the CDF for a specific point, we integrate from negative infinity up to that point.  \n",
    "\n",
    "\n",
    "When upon PDF we perform integration basically find area under the curve we get **CDF**.  \n",
    "And when we do **CDF differentiation** we get **PDF**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd5f813",
   "metadata": {},
   "source": [
    "## Use of Normal Distribution in Data Science\n",
    "\n",
    "### 1. Outlier Detection\n",
    "\n",
    "As we know one of the Normal Distribution properties is **â€˜Empirical Ruleâ€™** and as per **â€˜Empirical Ruleâ€™** if any datapoint is away for **â€˜+3, -3â€™** standard deviation then we count that point as **â€˜Outlierâ€™**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Assumptions on Data for ML Algorithms\n",
    "\n",
    "In machine learning, there are several algorithms like **â€˜Linear Regressionâ€™**, **â€˜Logistic Regressionâ€™**, **â€˜Gaussian Mixture Modelsâ€™** they take an assumptions that **â€˜Data is Normally Distributedâ€™**.  \n",
    "In Linear Regression we take normality assumptions upon **â€˜Residualsâ€™**.  \n",
    "\n",
    "So if we do not provide normally distributed data to these kind of algorithms its performance not much good.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Hypothesis Testing\n",
    "\n",
    "When making inferences about a population, many tests operate under the assumption that the data follows a normal distribution.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Centre Limit Theorem\n",
    "\n",
    "The core principle of the **Central Limit Theorem** is that regardless of the original distributionâ€™s shape, when we sample from it, the distribution of the sample means will tend to be normal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68b6990",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Credits\n",
    "\n",
    "**Prepared by:**  \n",
    "**Chetan Sharma**  \n",
    "VGEC | AIML / Data Science Notes  \n",
    "\n",
    "ðŸ”— **GitHub:** [github.com/Chetan559](https://github.com/Chetan559)  \n",
    "ðŸŒ **Portfolio:** [chetan559.github.io](https://chetan559.github.io)  \n",
    "ðŸ’¼ **LinkedIn:** [linkedin.com/in/sharma-chetan-k](https://www.linkedin.com/in/sharma-chetan-k/)  \n",
    "\n",
    "These notes were compiled for learning, revision, and academic understanding. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
